{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4271fb58362347e29210e06d6da958e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./dataset', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "testset = torchvision.datasets.CIFAR10(root='./dataset', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=4, \n",
    "                                        shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act):\n",
    "        super(MLP, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layer = n_layer\n",
    "        self.act = act\n",
    "        \n",
    "        self.fc = nn.Linear(self.in_dim, self.hid_dim)\n",
    "        \n",
    "        # nn.ModuleList?\n",
    "        self.linears = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.n_layer -1):\n",
    "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        if self.act =='relu':\n",
    "            self.act = nn.ReLU()\n",
    "    \n",
    "    ## forward?\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc(x))\n",
    "        for fc in self.linears:\n",
    "            x = self.act(fc(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = MLP(3072, 10, 100, 4, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args):\n",
    "  \n",
    "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act)\n",
    "    net.cuda()\n",
    "    print(net)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.mm)\n",
    "    \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        # ==== Train ===== #\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, 3072)\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            train_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "\n",
    "        # ==== Validation ====== #\n",
    "        net.eval()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                images, labels = data\n",
    "                images = images.view(-1, 3072)\n",
    "                \n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "                outputs = net(images)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss = val_loss / len(valloader)\n",
    "            val_acc = 100 * correct / total\n",
    "            \n",
    "        print('Epoch {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(epoch, train_loss, val_loss, val_acc ))\n",
    "\n",
    "\n",
    "    # ===== Evaluation ===== #\n",
    "    net.eval()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.view(-1, 3072)\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "            \n",
    "    return train_loss, val_loss, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=50, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: 16.506\n",
      "[1,  4000] loss: 323.983\n",
      "[1,  6000] loss: 195.345\n",
      "[1,  8000] loss: 287.030\n",
      "[1, 10000] loss: 378.851\n",
      "Epoch 0, Train Loss: 2403431.551299691, Val Loss: 452.247026071167, Val Acc: 9.33\n",
      "[2,  2000] loss: 355.651\n",
      "[2,  4000] loss: 432.978\n",
      "[2,  6000] loss: 483.307\n",
      "[2,  8000] loss: 603.673\n",
      "[2, 10000] loss: 707.553\n",
      "Epoch 1, Train Loss: 5166326.8269844055, Val Loss: 861.3773551147461, Val Acc: 10.21\n",
      "(5166326.8269844055, 861.3773551147461, 10.21, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=150, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (1): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (2): Linear(in_features=150, out_features=150, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=150, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=50, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: 28046395.876\n",
      "[1,  4000] loss: 98.621\n",
      "[1,  6000] loss: 201.909\n",
      "[1,  8000] loss: 309.206\n",
      "[1, 10000] loss: 396.898\n",
      "Epoch 0, Train Loss: 56094805020.88416, Val Loss: 606.5786016540527, Val Acc: 10.18\n",
      "[2,  2000] loss: 443.644\n",
      "[2,  4000] loss: 577.784\n",
      "[2,  6000] loss: 683.279\n",
      "[2,  8000] loss: 807.265\n",
      "[2, 10000] loss: 914.482\n",
      "Epoch 1, Train Loss: 6852908.027637482, Val Loss: 993.4194247680664, Val Acc: 10.18\n",
      "(6852908.027637482, 993.4194247680664, 10.18, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=150, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (1): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (2): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=150, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=50, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: 12.806\n",
      "[1,  4000] loss: 79.922\n",
      "[1,  6000] loss: 171.718\n",
      "[1,  8000] loss: 275.064\n",
      "[1, 10000] loss: 364.874\n",
      "Epoch 0, Train Loss: 1808765.4929199219, Val Loss: 528.3172410491943, Val Acc: 10.21\n",
      "[2,  2000] loss: 395.450\n",
      "[2,  4000] loss: 485.598\n",
      "[2,  6000] loss: 713.492\n",
      "[2,  8000] loss: 655.697\n",
      "[2, 10000] loss: 771.625\n",
      "Epoch 1, Train Loss: 6043721.131706238, Val Loss: 784.7913546905518, Val Acc: 9.97\n",
      "(6043721.131706238, 784.7913546905518, 9.97, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=100, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n",
      "MLP(\n",
      "  (fc): Linear(in_features=3072, out_features=150, bias=True)\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (1): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (2): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (4): Linear(in_features=150, out_features=150, bias=True)\n",
      "  )\n",
      "  (fc2): Linear(in_features=150, out_features=10, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n",
      "[1,  2000] loss: nan\n",
      "[1,  4000] loss: nan\n",
      "[1,  6000] loss: nan\n",
      "[1,  8000] loss: nan\n",
      "[1, 10000] loss: nan\n",
      "Epoch 0, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "[2,  2000] loss: nan\n",
      "[2,  4000] loss: nan\n",
      "[2,  6000] loss: nan\n",
      "[2,  8000] loss: nan\n",
      "[2, 10000] loss: nan\n",
      "Epoch 1, Train Loss: nan, Val Loss: nan, Val Acc: 10.55\n",
      "(nan, nan, 10.55, 10.0)\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "args.n_layer = 5\n",
    "args.in_dim = 3072\n",
    "args.out_dim = 10\n",
    "args.hid_dim = 100\n",
    "args.act = 'relu'\n",
    "\n",
    "args.lr = 0.001\n",
    "args.mm = 0.9\n",
    "args.epoch = 2\n",
    "\n",
    "\n",
    "list_var1 = [4, 5, 6]\n",
    "list_var2 = [50, 100, 150]\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        args.n_layer = var1\n",
    "        args.hid_dim = var2\n",
    "        result = experiment(args)\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
