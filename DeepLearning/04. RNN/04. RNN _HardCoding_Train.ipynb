{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karpathy's preprocessing\n",
    "data = \"hihello\"\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(1, 240, 28), dtype=float32) Tensor(\"one_hot_1:0\", shape=(1, 240, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# hihello 를 list로 바꾸면 중복제거되서 shape가 5\n",
    "print(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 0.19925487 \n",
      "pred : hiieeii \n",
      " --------------------------------------------------\n",
      "step : 50 cost : 1.0066944 \n",
      "pred : hieelii \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 0.5478621 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 150 cost : 0.31220034 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 0.1961207 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 250 cost : 0.13218275 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 0.09447693 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 350 cost : 0.0708718 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 400 cost : 0.0552492 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 450 cost : 0.04439844 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 500 cost : 0.036551174 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 550 cost : 0.030682765 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 0.026170468 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 650 cost : 0.022619313 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 700 cost : 0.019769115 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 750 cost : 0.017442936 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 800 cost : 0.015516867 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 850 cost : 0.013901964 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 0.012532983 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 950 cost : 0.011361222 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 1000 cost : 0.01034954 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(1001):\n",
    "        #Test\n",
    "        if step % 50 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
