{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = { }\n",
    "test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-111b3b61b130>, line 198)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-111b3b61b130>\"\u001b[1;36m, line \u001b[1;32m198\u001b[0m\n\u001b[1;33m    smooth_loss = loss\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "\n",
    "BSD License\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# data I/O\n",
    "\n",
    "data = 'The first appearance of an Infinity Gem occurred in 1972 in Marvel Premiere #1. It was originally called a \"Soul Gem.\"[1][2] In 1976, a second \"Soul Gem\" appeared in a Captain Marvel story and established that there were six Soul Gems, each with different powers.' # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "print('Original Text :', data, '\\n')\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "hidden_size = 5 # size of hidden layer of neurons\n",
    "\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  inputs,targets are both list of integers.\n",
    "\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "\n",
    "  hs[-1] = np.copy(hprev)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  # forward pass\n",
    "\n",
    "  for t in range(len(inputs)):\n",
    "\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "\n",
    "    xs[t][inputs[t]] = 1\n",
    "\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "  # backward pass: compute gradients going backwards\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "  for t in reversed(range(len(inputs))):\n",
    "\n",
    "    dy = np.copy(ps[t])\n",
    "\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "\n",
    "    dby += dy\n",
    "\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "\n",
    "    dbh += dhraw\n",
    "\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "\n",
    "  x[seed_ix] = 1\n",
    "\n",
    "  ixes = []\n",
    "\n",
    "  for t in range(n):\n",
    "\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "\n",
    "    y = np.dot(Why, h) + by\n",
    "\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "    ix = np.argmax(p)\n",
    "\n",
    "    #ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "\n",
    "    x[ix] = 1\n",
    "\n",
    "    ixes.append(ix)\n",
    "\n",
    "  return ixes\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "\n",
    "    p = 0 # go from start of data\n",
    "\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "\n",
    "  # sample from the model now and then\n",
    "\n",
    "  if n % 100 == 0:\n",
    "\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "  \n",
    "\n",
    "  # perform parameter update with Adagrad\n",
    "\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "\n",
    "    mem += dparam * dparam\n",
    "\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "\n",
    "  n += 1 # iteration counte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 263 characters, 41 unique.\n",
      "Original Text : The first appearance of an Infinity Gem occurred in 1972 in Marvel Premiere #1. It was originally called a \"Soul Gem.\"[1][2] In 1976, a second \"Soul Gem\" appeared in a Captain Marvel story and established that there were six Soul Gems, each with different powers. \n",
      "\n",
      "----\n",
      " upbucummT,.TxulMP9xdrne]]xvTeSsrmvhx.eS\"gop pGht\"6SCvyT7evsg]v],,,du9fupP,tIv]v7dxe.9ssupe,lTfx7rx..]wcyTP\"aw#7[ uobga[9]171T#]iph7Mp2M]PTmelnx.6hMogiwC T[,SiSw e1.7bw1vPtsyG6mInG1eP2I.\"ddha1td]u6h6cm \n",
      "----\n",
      "iter 0, loss: 92.838330\n",
      "----\n",
      " Cd Is em isrefvl [SaMfI Mip\" .r17C76a tidl aiTPid isetn o.y ietib,e v1Cn Ctplnra xr]fre we o Sstid Me e ihMeib[]lredrera ffenpeae 7ylllM\"l G hb nSoGalml 2 pi.mteme i wen ]cln PgareTae en efia, M Get G \n",
      "----\n",
      "iter 100, loss: 62.007134\n",
      "----\n",
      "  #h m s972 Geml o  1792vioGht C 9wyd e Mal t9.In on an an.\"cor 9I \"fen ol 1ulpara sSald hariId #sta 1676t aSIl MIymf ap a Cn e Geudl GerGed sa 1v Cy en on e]\"9\"bm]cst Plred x,l s17pPt o\" fer  wyri[Pfe \n",
      "----\n",
      "iter 200, loss: 56.696466\n",
      "----\n",
      " hed \"1]ns 1992cel Gel ad sved wa2  oI a]ol 192mGGen1.dhen In \"2d Ma siS19ItfeiraGe moMerd 1elpvemsear t 97[#7rvarf wmldePi inl Sm2laitmInlinsIiGesIt omler ore in col Gen ##7velrMan Ge \"ir9omwer1oceced \n",
      "----\n",
      "iter 300, loss: 52.729655\n",
      "----\n",
      "  o. ecurf ar7n #16vera #aumGem,alir, tamSa] Indd e s a]7ihi 1o2ichad omroumfe in ssGerurs 1976s Maul Ii#vemSaumueredl Iw[Id sCored Ml Ghl al an heddl a.\"heles ed temSan fMcrad femphcerce s.a Ppee Caad \n",
      "----\n",
      "iter 400, loss: 49.825492\n",
      "----\n",
      " Soubl Purbif an I Cop7shel I[Ip.ineel tSoSo2 sh orh Gel inthe d he inia sen in warvariccecumthera dicel alvalffirIiresrel altat In CarefcIy warere lharirIcehaa of eau we .tauu .]ind Gemthtcol eniitham \n",
      "----\n",
      "iter 500, loss: 47.946297\n",
      "----\n",
      " hed wain Ge e shef arein In Pf amp1Ire ix7#hl avend ed \" Im1Sored \"91]S1[2p an saw theilSos iniI Pur inetth \"[.lhel \"hcein then Ge \"nel tla. Inineoreocar taud shiw Syl e In In Caul wind sad ed t#Clp,  \n",
      "----\n",
      "iter 600, loss: 46.722071\n",
      "----\n",
      " hamphat taren a oory si.thel ty a f Md ily sarecrl GerinithefiGad Comy wiryled wecs al9G9ul fet sinpsd fel s1u.lesrigref an \"hvan ir. Ind iny a [,pvel Pbp.oul Cntten ia2limgafcef en ore ta. , 1pul e 2 \n",
      "----\n",
      "iter 700, loss: 45.854427\n",
      "----\n",
      " he thedfycen eab he sinis.thcc9x antere G oren ty verslathdd vax eainre iixSwfveafced oainirylit o] thestax 192] hen IsthemsSol sin \"976py tac1]lhem otthe Prim.Minl] weul Gel ot Ini#2l Ger dithimnarve \n",
      "----\n",
      "iter 800, loss: 45.178534\n",
      "----\n",
      " y fe It#ompGers e 72pi winy 1hllet ocom Il shee shet Ins,i1h occ]nGel Med ane wax 1976Iisshecrem aSory Gedl ol Mam Id intere there wan wen Id ov \"y sin ey e x InisSap ere s,pMaumull oc] 197Mert Gen ib \n",
      "----\n",
      "iter 900, loss: 44.602054\n",
      "----\n",
      "  at waddifecf raffeifian #9up, 2dd earen Im\"hy oarGel six oumc,ifin oemty s oulpin aS97.p\" w MarvatisIs In orgrGed sag shan eorveml,pharfed a \"Saptha arGem Gim [Pul Gemry a sac]nwel wcrlp areesi 1rurr \n",
      "----\n",
      "iter 1000, loss: 44.073485\n",
      "----\n",
      " hvamthd fe firGemfearem.nareis \"nitas ware \" 17ul a alved #h] TIw at al a thd a 1oul eem\" orvem.ffin iof Inity eared hen Idadire albn a Gyrin co2 eetashe #1pp, \"9S9#1ulteis.heb remthem In os eas P[ne  \n",
      "----\n",
      "iter 1100, loss: 43.720806\n",
      "----\n",
      " Soul cnn In Ght an as.MamCwan any wertaf an Mos shsfstas th orecarfe ai#Is eem 19ul Gers ore we fvel Pf a sharfirind Geri Cax[\"pbl al waPsainin Mare ffteninabct tSory in hed awiaud oed GenmGel Prerinr \n",
      "----\n",
      "iter 1200, loss: 43.268802\n",
      "----\n",
      " heifiin Ifeemsecwlcsteinin \"[7[7#97sSa6vvan sar]asMaccvl wemMan inloc7c7l sin 197.Sypsam2edit 7S97[[91plegreilhe or1mpvecrcal recu] Ca2 shfinarercarfcet inna ty in In G]n Garin Man eab werim2en \"heacM \n",
      "----\n",
      "iter 1300, loss: 42.867883\n",
      "----\n",
      " 9wl,.ne ty anit hl ty camseirvvcamp1]]Serpfe  \"ppI insererrarse tore nacSa2 ic7upue finithdife inpeere anrarsin orel CapSaul a Gel Gem In Gemtery omtary Ithedd anai.\"hirl rem\"Soul Gemthstac][hvam.oGec \n",
      "----\n",
      "iter 1400, loss: 42.605025\n",
      "----\n",
      " Sap em\"y fe Ged a edreis hecg thfs\"tht a.Stnitwset Ina os \"h id tdrtar wiriaulp#auurinhven #oul eai Gh in or  xl,pMar af an eareareren anren Intaul eam#77xpCar orcumlSoupiacflereiread Gerredifin Gvarf \n",
      "----\n",
      "iter 1500, loss: 42.450863\n",
      "----\n",
      "  1ld Ghd ania..iap egredrGem ap, ecomut al a[7p, coin 1972 eapin\" orpsterette  oul eere wa fGemfea in iougtaremt,lerin ferin ai7\", earvac\"l a #97. , Iix 191l hed Marhed antanmfecapied ta. Iftaintertn  \n",
      "----\n",
      "iter 1600, loss: 42.073842\n",
      "----\n",
      " hed tarera #a. , Cos 1][2] Mem6, Ger ab Geifth rersnai alhel a wacpp, antiwtec92l fcem.[17[\"]Soref ar, \"Soulle syy Gem6Cargemsen at om\" on a \"[7[Istertireed sfsin 1]lhercortasfefincalled shet 19976v a \n",
      "----\n",
      "iter 1700, loss: 41.946875\n",
      "----\n",
      "  M][19CSn vhd c1ly ediii117. I1Soull apv arf et wel thramthin orersal shere aSe2 in Mhmtabl orim.,ioslabm#d soxdd \"[]h arid Gem.CocefineitSnplhiri I\"Sal egigcIt Initdmth f an In \"[72 th rsi 1972 ea a[ \n",
      "----\n",
      "iter 1800, loss: 41.602201\n",
      "----\n",
      " hvenifiaread ,lar snity e al th2d a Insa thve #1uunfeir] Pul sarei thad aaiagared fet Mecul taul e \"l.\"S9[2ll s omSorefted in sSoul a 1y in 191lptef an , Gylared and ta. \"976eemsMaap ain abd wed terh  \n",
      "----\n",
      "iter 1900, loss: 41.504825\n",
      "----\n",
      " y sarel igre m[,seeree a[.So. CappGemp, ioul GampIiSoul in anid cal effe be aireed anin warvery anre ty Gem.sCalpearal #1p at a 1][2ll Ircen sared in Gy seniithgan orerintoret in in \" 6pe Inin sy Gemt \n",
      "----\n",
      "iter 2000, loss: 41.464962\n",
      "----\n",
      " y an an ix.MamSos et o. thigityd a 1hl Gedd sarecc][6vem orin a 1oul \"nin a[, ap arhel s[a[. hed in ocgmtSou c, Gem\" w oum.as, an ty al Precudi 1op ar, ia. Marvere thamlhamfeed threl Pd sar allen Mac] \n",
      "----\n",
      "iter 2100, loss: 41.232520\n",
      "----\n",
      " a. Cap Mal , G9ullein a\"pGvem,, are sfis#,thrcwmfere ry a ty in\",lerfreld in It 1]l.\"Soplared hamly Gel anit 19Cw. Mas #91ppin a orarin ore inanimSoul Gel a thin add anin In e xSol Gerred in al.Ithimp \n",
      "----\n",
      "iter 2200, loss: 40.980373\n",
      "----\n",
      " y inham \"91ll en taum am[S92l in Inta. 19.S[7.\"C[976, e aly arsaf ain wel I 1arvemth rved I 197[Iorel a she #v, aa or crpfeirein G rll Ged \" 1hl a wamSorere fed eaxniaupiarvel wamseam ocapty in I I Io \n",
      "----\n",
      "iter 2300, loss: 40.909820\n",
      "----\n",
      " y Gemseimny sared a ty e inpafseiopten eare n Maul sec1lptemp, ein ar,ffen shaf al Mab f eaf amt,.as anlen an\"d sam[Iareim.sC1p]heif an Mac7.pIafsin Cappeain th r] In \"S[2 f etafe in s]h,aithimt x[72  \n",
      "----\n",
      "iter 2400, loss: 40.845141\n",
      "----\n",
      " Snlleim2y e w ocrm[hvercam.hemseed ,Cin Macppseis Inheireim\",terinddd Iithams1um.\"i6, \"[Soul weg \"[7bl Gercan Macul GemSaup#abmpf apI erled , Iniifuam Mabl Mac7p,yy a Ithecare feifiau it orbmiaS972 ol \n",
      "----\n",
      "iter 2500, loss: 40.803528\n",
      "----\n",
      "  1[2vverin at w wt 1pu corereld sas aSop weral Men fad a sy Gercarrem Ir, wercem.\"SIupiar,ita2 an en sy Prid soreirhenisty o b eixSoum.heltMerrel a 197p.yved dd in oim\"\"pias Intadd \"Soul Ged feced ear \n",
      "----\n",
      "iter 2600, loss: 40.851643\n",
      "----\n",
      " h al Gem.Mabl eacvem\" \"[wvemiap,Cead sory ia.\"hen \" 1][It G]n warverinisInared farred sary apiaul etd thd inpGemty fim.17.Shialeftecc][2] In wan \"n arversren sarvem, Mabd f ap.thn Maredleis,\"ylercad i \n",
      "----\n",
      "iter 2700, loss: 40.822365\n",
      "----\n",
      " Sappcal Gen #oul eas apCert apued a P\" , \"p. 1ob intab ty edsi w occ][p \"[72ylemiatthinnid tht feran thveiin ol Gem\"ecamsI tSapian tvve lp, etn al at enithim.hedisored war ocpftare a sy Gerletarvendi  \n",
      "----\n",
      "iter 2800, loss: 40.922740\n",
      "----\n",
      " y e Min 1976, e ws#hl sac] yy eir, #apiam\"tharadisMar am2ers Mathed cog ty edleffeiin omps x9Ip, ar\" fdifein theigaisIa 191llleg tht saCored \" 1wry Gem.f, Gempearafd asram.172lecc92piashenisMar adif a \n",
      "----\n",
      "iter 2900, loss: 40.563105\n",
      "----\n",
      " yvin earvemiar, ee In 7[Ix \"92ll ca] \"972lered a \"[197#. Ic76eemP arverinnan saursel Maul ee edd in a Coreretdlegis\"vyeeiffamsvy com 1976Gempeerin \"l76verfasenlared tab sarylel a thdic, eored tebnista \n",
      "----\n",
      "iter 3000, loss: 40.872613\n",
      "----\n",
      " ]\"Soul Gerf Prhdi an anit wt wap In coul anced in \"[2ll Gercd inas an ein Insod e op agitvyy an an orefithdatal In a[Cored anted afiad sar,fsarigs#oullarfin wax In oum.\"[Mhn war an Gy Mar f we fierare \n",
      "----\n",
      "iter 3100, loss: 40.607017\n",
      "----\n",
      " hin ansacal Gercem ap.apty e wixSoremiar, wamthccll at w 19C[2limiet shy sa#S97.sI c76, Gert It wem.Pcylad a I.Cappean Men sh in Intar omis\"piarcaump, 1976,d sfinin wac. Soul e o. Intary f al GemSarir \n",
      "----\n",
      "iter 3200, loss: 40.978653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " heffillPrveg th oin Maf eararind tapsiappeecGem 192ll erin MacCor d tobl a .1ullad \"l1][1]2ppeirv]nemth \".\"fifcGsl al96v] In or acupse #92ll od taremt opiarel a \"Soul efifih an tnta#apsar ax herin G]d \n",
      "----\n",
      "iter 3300, loss: 40.932176\n",
      "----\n",
      " 1urs s st]yped werf t wnareir, 1pupsed \"[, \"]Shl ty ea apSo. Puc. In wabl Main eere it976, e c76, 1]2l in In sher ab shan sa6, ocw taul Gerf a thveioia\"had in \"p.s\"]S1uprarf an in in e w \"[1pp,i a6,ir \n",
      "----\n",
      "iter 3400, loss: 40.648287\n",
      "----\n",
      " he shem an Gem\"Soul anirGedmsar wfsal earertis6ead fere Preg odithim Mau s Mabl earerfi#Cos G cn a #Soul Gemiar]a#[72 cor cal , Gy eirvel Gempe xS7x 1op odlare fiddlein wareitaseccorhid terel ty in sa \n",
      "----\n",
      "iter 3500, loss: 40.622014\n",
      "----\n",
      " heind Man Ininfa war,itaremscalce f am\"yy fe tap Marvefsed fed a Ini#[6, e wshan \" 6, a 1oul Pd wams, Mar arurcarpit xn agiathad terlarintaf eaf et \"p, an ani ocG]ppenfsacumihGarrein InaisIfsem\"v, tap \n",
      "----\n",
      "iter 3600, loss: 40.491935\n",
      "----\n",
      " hin Pcerrarf a sSory e a Inin In 19ul in od thl \" 2]l Gy in tegdcGemlhvin Gved thims1]l[Cal i#2psared I Ithel at s 1977Shlle narved \"SourGem.\"]s, In \"ppSo. \"Sg.pIin 1]pSorvel Sory e wtaul Gemty an en  \n",
      "----\n",
      "iter 3700, loss: 40.588925\n",
      "----\n",
      " he th xth a]hvemsee ity Gvcc]S972lel Ca.Sor rrm \"S976, fe Mad a Corledd a thdra in It , Igfvedd fere thimvvemiend Gverin a[1]n ally G]n It 1976yvegisthdred thd s1] Cor,f ap an ehd MarvyrGem6, Gy Gy a  \n",
      "----\n",
      "iter 3800, loss: 40.439172\n",
      "----\n",
      "  opiaun thccmp t1ul sared \"[1976v cr al Martaredditarecw in 1976, aninaren thld in 1976, a shediitharGem h gm \"pp,,\" w themsalles\" In a 2]nerel Mat ty Gamty eed eix \"972 Gel she a 1976, in \"Sx2p#a[2er \n",
      "----\n",
      "iter 3900, loss: 40.610216\n",
      "----\n",
      " y cal in a 17Cvvertaineis Sapseareaitary Gerrenfios am,,iad \"[, Infiaptar om In war codnfi ol Gersdte Man Gerintenifceds\" wt\"ly Gedre ty e wtnain 2pfel aheeftad Mecp iniferad fe tSoul eamy ccp in wem  \n",
      "----\n",
      "iter 4000, loss: 40.839079\n",
      "----\n",
      " he a odd sary al sarere sIsthan in 19,pta#[I6vempaniithed Intarhdndlam.In opiareg idrin on \" S][p1][ol Glrs Pb s\" C]2ptarl or ccul Gend Gylem en Mac]Soul GemSaupias hemue Mary e x 1972 in earylel saul \n",
      "----\n",
      "iter 4100, loss: 41.013401\n",
      "----\n",
      " ored saly Gerredlared anat ol Macul in Cos Mary ed 17upiaul Gel Gy in Marven d alled It olvvers anisereit ogfi apian eared \" 19ull all all alleisserdd Prercem.I\".Sop en a IisCal Pry wel ioul en tnad w \n",
      "----\n",
      "iter 4200, loss: 41.767527\n",
      "----\n",
      " ered tt]S976, torvel chl i 6vears winareliren eashed corh #op 17.2y dmter ap ar, fepfan tabl h a Pthair, Gy corlestat Indesin \"n en ta. In Marvem emCag Inisedl shir ocerin \"n amSal \"[2] 1976, Gy e xgi \n",
      "----\n",
      "iter 4300, loss: 40.742555\n",
      "----\n",
      " Sapiarfe \" 2]Sol Interftere thccmp.If,,itaul Gex wasieasvvemued in eary G]n eaul wemty coin in \"Soul e x Cal aireldlad 1976, feftar arrediinod wam.Marerel a 1][.Sourigiap1][2] In a[hh lSoursein was sh \n",
      "----\n",
      "iter 4400, loss: 40.301480\n",
      "----\n",
      "  1[2l sasetecul sa#[1][7S972 al wn alSoumpPm 197blledleit 199972 whffersabrGem eap an in in alCamia t wn an an ancGempGy afiary s \"nlerin Malheare sty wempan a[Cal Ind ed \" 197pSebre #oupfecc972 calpe \n",
      "----\n",
      "iter 4500, loss: 40.284548\n",
      "----\n",
      "  wnd a Prel Pcal sestem MabimSoumpGampsarved in a[Corlerelare t x[.hy G]l,lel Inin ap,lerf ad Gvin in sharad in a of al 1976, a w1] 17p\"ounisan eniithesi wneddd in eare gth aptary , Mabl sarverin Marv \n",
      "----\n",
      "iter 4600, loss: 40.380598\n",
      "----\n",
      " heredfi w Pry wed saullen taul eam,temit ol e r]p\"y calleredlenisMamty corid thl sSoul eas alpGem In saul GarcerGy earamias,, MarimpIntereirin sareaifigth wii#172 Gem \"[C][2] 1][.SoGveredisad en a sIf \n",
      "----\n",
      "iter 4700, loss: 40.392220\n",
      "----\n",
      "  or\" eared \"heeaid \" S]2psarin thcG] vers apsedd tMemithd serha s\" 2vpearereiredith wsIndseag 1oul Gellereminin sy cal Gempeam\".hercollhe w a.p1][S97..ItSo. ocalpinaceoremthd Mar a] hed ty ed shecal e \n",
      "----\n",
      "iter 4800, loss: 40.712155\n",
      "----\n",
      " he Pbnh i[2 w a Malheareirim yy et shecGvpfe stherfttd a Intaressafsemfemsac72 e wthireafa shemiin In#197x Cor acullel \"2pin alyy Iniwserrercamia.\", In ty GarGverin a 2veccor al fere torved cwt ea.the \n",
      "----\n",
      "iter 4900, loss: 40.844740\n",
      "----\n",
      " he tob shegd anefia.Soulpcal sapSauure t 72 Gacul 1orug MamSa6,ftarvel ty Garrediineit oarin ty eaGvecrecc][2]piax \"97p2y an abdcGemp, Inithed Inin In apiarelcc]n entared eas Mabl Gem\"hed con femthd f \n",
      "----\n",
      "iter 5000, loss: 48.671539\n",
      "----\n",
      "  olledd en al c][29w tIrvadtt a[Canife wereirecw a In og a 1]976, areared ttlll Gyleit 1]6, a in \"[76,lacc]lty G]n 1oupsix[Sorecw thccmullertaremiaryemthd in a 6, e wMan \"[In Inithal 1972 cal in c1uup \n",
      "----\n",
      "iter 5100, loss: 41.985497\n",
      "----\n",
      "  ol fed fen al wn a 1976, \" 2veamit w Ia.Cwinifi s, In shad a syylad \"Sopiastar al \" 19i[6, a than et wix Intirursed in \"n a .Shlleccorlereisithaddity earen Pcvemfedn a tylad #1ul idlte the nary ad a  \n",
      "----\n",
      "iter 5200, loss: 41.087187\n",
      "----\n",
      " yy coll orw a 6, a ty eirlally e wtap warirs c972 #oc]l1]n a[Cord sem\"\"hearercemiaiithel was a[Itnin \"[1][S96v cl]\"Inemty e t 1#2 sam In a S976,in thims, Gvartaoinarcall ec2 a s wn Pn shal Inta#2uret  \n",
      "----\n",
      "iter 5300, loss: 41.458428\n",
      "----\n",
      " hedd \" olhen in Marylerf an a 2y earel Morverin shcwl Gvccm.\"weag a Soul Gel in oarinithad shlaina ixn arumpead tab ty Gecrefi al\"pMenife a sSorvelt t[hy e wtec72 remfeal wan \"l] 1]ntenitefvverswed ea \n",
      "----\n",
      "iter 5400, loss: 50.381752\n",
      "----\n",
      " y G]n 11. In olithe thefiathdd th c72 sarvegitasv, a Mer am.ty Gal shfine atexnin Prved shin eecMempin wac72 #oul a a . w 2veem\"y alpia[hera oe Maculled shin eedt apsedt o n a[2v agnel th n 1976, in M \n",
      "----\n",
      "iter 5500, loss: 42.005053\n",
      "----\n",
      " he itSoum\", an tervel Ind tstix In a2re read sar,larin It GvpsasfeacrempMal o n \" 2lledisen in \"[G]p\" \"972 sar,taralarse thad a 1916bessel shacw shy eaum al a 2y e wd al G][2] \"SnpGel Gy a in 1976Gel  \n",
      "----\n",
      "iter 5600, loss: 41.499209\n",
      "----\n",
      " heftaf e wthad tap al 1972 Gemshel capfe x 192ll al sare nagaty Gy Gerf easyval Gy eein alyved in In\" 1976, in Ind om a6, e x IgPinafe t Corh in \"l 6, wemtvvempeeccort apsel verimsInfiareffe fvenreesi \n",
      "----\n",
      "iter 5700, loss: 42.496706\n",
      "----\n",
      " he t17p\"hertar allett wtes aSoul ,ne \"niattedled sarval eacemial sorerremthcc]S976veirid taul Gen in aSoul Gelpeere there ntacerin ioul earedirem ax Ingad sabl warffi#xl17upih saread thdt a Pbrsarledi \n",
      "----\n",
      "iter 5800, loss: 43.524973\n",
      "----\n",
      " he ed onffaaffe th wsan Macply GemiPrepfen ty an apfemtar emia 1972 oid a \"[76, e shy an a \" 2]Soul Inin eaty G]gf, MamCemiasseem.191ll eafterd eac\"n in a sherdiinha 176,ired ty Gemiat apfed shirrel i \n",
      "----\n",
      "iter 5900, loss: 43.855703\n",
      "----\n",
      " hen th a Sobld earel \" Coc] In ocG] p\"].\"Ithe thn \"[hy coudlesit w x Inif, \" 2premi oryhcal s1um.tylerd a a Saupin a \"Snllen a \"h earertererinisente 176versainin Mer arin occ]Soul Gem.thare tec] Corhe \n",
      "----\n",
      "iter 6000, loss: 42.758219\n",
      "----\n",
      "  wn 19S[6eem 1Sn in \"[1]pCecrea al 19y Gerred #apvel \" My ta6, caps, It 1972pGerd Marim.In x 191p\" Prere sSory e w\"So. In wain ear all w thdn taptera#a 2y ioc] Inei t976, Pry ed th mSorpcallein in #ob \n",
      "----\n",
      "iter 6100, loss: 42.369300\n",
      "----\n",
      "  1]sIob wegneccorhfin apsvem, aginasIn eis\"threcrved \" 1][\"p.192ll edld ia6eerfteim\"her a.\"1][19] Corereire wal \"pId eas a]sherin \"97.\"vy am\" app, Mary ampa fyvacw shdn a \"[,peareistabl shed sery Gem. \n",
      "----\n",
      "iter 6200, loss: 42.705110\n",
      "----\n",
      "  wn in anitha end a wam.\"Souv, eaineister cm.thad she stor omp, x 1][2]l Marvecbrteren In aSwt sSoul fel shrcemthamty eere in 1oul in a[hecc][pCol sarveg apiecr][Soul Gem.Ithe socapiatheccor erGempcal \n",
      "----\n",
      "iter 6300, loss: 42.347945\n",
      "----\n",
      " y en Gy eefit1ullim.Inedim\"Carren aty calle ty in \"wfel thed eaf adsin 1972 G]nseccwrheseeradiseasfcampeirped s1ulled Gy in Pf in a waculleafen al w wan eared shasfad eab thereif a#6, Mar am\"y crurmee \n",
      "----\n",
      "iter 6400, loss: 44.500682\n",
      "----\n",
      " he gaupiasharinithasinler or cmpGegiithare #972 a x Cof eat wabmiareil in Ini#sy wn Mary]th ren alve x Ity corireirind feiit 1976,fit ollel tapiaigf, 1976, a \" Is\" appe oul Pm 197.sSoul Gerrercarrem#y \n",
      "----\n",
      "iter 6500, loss: 42.674987\n",
      "----\n",
      " y G]nia th mpItninnarGam.1Soul earerin G] .2ed \" Inith rum Mar, hem\"yy alpGarinity Ghdd th n shemita.\"\"hemfeacc]S[76vel a Preria. Iap, a \" Capi opi1ulleis\"him 17.\"Ihet al Ine eabel \"[yhedre c7bl e P.  \n",
      "----\n",
      "iter 6600, loss: 42.616058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b522e7ebe946>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    193\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;31m#   smooth_loss = smooth_loss * 0.999 + loss * 0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b522e7ebe946>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mby\u001b[0m \u001b[1;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# probabilities for next chars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "\n",
    "BSD License\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# data I/O\n",
    "\n",
    "data = 'The first appearance of an Infinity Gem occurred in 1972 in Marvel Premiere #1. It was originally called a \"Soul Gem.\"[1][2] In 1976, a second \"Soul Gem\" appeared in a Captain Marvel story and established that there were six Soul Gems, each with different powers.' # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "print('Original Text :', data, '\\n')\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "hidden_size = 5 # size of hidden layer of neurons\n",
    "\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  inputs,targets are both list of integers.\n",
    "\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "\n",
    "  hs[-1] = np.copy(hprev)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  # forward pass\n",
    "\n",
    "  for t in range(len(inputs)):\n",
    "\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "\n",
    "    xs[t][inputs[t]] = 1\n",
    "\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "  # backward pass: compute gradients going backwards\n",
    "\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "  for t in reversed(range(len(inputs))):\n",
    "\n",
    "    dy = np.copy(ps[t])\n",
    "\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "\n",
    "    dby += dy\n",
    "\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "\n",
    "    dbh += dhraw\n",
    "\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "\n",
    "  x[seed_ix] = 1\n",
    "\n",
    "  ixes = []\n",
    "\n",
    "  for t in range(n):\n",
    "\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "\n",
    "    y = np.dot(Why, h) + by\n",
    "\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "#     ix = np.argmax(p)\n",
    "\n",
    "    ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "\n",
    "    x[ix] = 1\n",
    "\n",
    "    ixes.append(ix)\n",
    "\n",
    "  return ixes\n",
    "\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "\n",
    "    p = 0 # go from start of data\n",
    "\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "\n",
    "\n",
    "  # sample from the model now and then\n",
    "\n",
    "  if n % 100 == 0:\n",
    "\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "\n",
    "#   smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  smooth_loss = loss\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "  \n",
    "\n",
    "  # perform parameter update with Adagrad\n",
    "\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "\n",
    "    mem += dparam * dparam\n",
    "\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "\n",
    "  n += 1 # iteration counte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
