{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic of RNN-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[-1.1559641 ]\n",
      " [-0.06461716]\n",
      " [ 0.81309044]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0.]\n",
      "------------------------------\n",
      "initial state:\n",
      " [[0.]] \n",
      "X_data val:\n",
      " [[[0. 1.]]] \n",
      "output val:\n",
      " [[[-0.06452737]]] \n",
      "state val:\n",
      " [[-0.06452737]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "X_data = np.array([[[0., 1.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 1\n",
    "X = tf.placeholder(tf.float32, [None, 1, 2])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, \n",
    "                                    initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names =[v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k,'\\n', v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs, state], feed_dict={X:X_data})\n",
    "    print('initial state:\\n', sess.run(initial_state),\n",
    "          '\\nX_data val:\\n', X_data,\n",
    "          '\\noutput val:\\n', outputs_val,\n",
    "          '\\nstate val:\\n', state_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic of RNN-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[ 0.58285165  0.67652035]\n",
      " [ 0.55150366 -0.8950167 ]\n",
      " [ 0.60206723  0.32385635]\n",
      " [ 0.1208539  -0.03462529]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0. 0.]\n",
      "------------------------------\n",
      "initial state:\n",
      " [[0. 0.]] \n",
      "X_data val:\n",
      " [[[0. 1.]\n",
      "  [1. 0.]]] \n",
      "output val:\n",
      " [[[ 0.50164634 -0.7138628 ]\n",
      "  [ 0.6632553   0.69815886]]] \n",
      "state val:\n",
      " [[0.6632553  0.69815886]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "X_data = np.array([[[0., 1.],[1., 0.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 2\n",
    "X = tf.placeholder(tf.float32, [None, 2, 2])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, \n",
    "                                    initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names =[v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k,'\\n', v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs, state], feed_dict={X:X_data})\n",
    "    print('initial state:\\n', sess.run(initial_state),\n",
    "          '\\nX_data val:\\n', X_data,\n",
    "          '\\noutput val:\\n', outputs_val,\n",
    "          '\\nstate val:\\n', state_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction of Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhlllii\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "data = 'hihello'\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "X_data = [char_to_ix[ch] for ch in data]\n",
    "X_onehot = tf.one_hot(X_data, vocab_size)\n",
    "\n",
    "hidden_size = 10\n",
    "seq_length = len(data)-1\n",
    "batch_size = 1\n",
    "\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 1, vocab_size])\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_test = sess.run(X_onehot[0]).reshape(1, vocab_size)\n",
    "    predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "    print(data[0] + seqtostr(predtxt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : hhooeei \n",
      " --------------------------------------------------\n",
      "step : 50 cost : 1.1324364 \n",
      "pred : hihehih \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 0.5456131 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 150 cost : 0.3089082 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 0.19208938 \n",
      "pred : hihello \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = \"hihello\"\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(201):\n",
    "        #Test\n",
    "        if step % 50 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
