{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "이 곳에 작성된 소스코드는 https://github.com/czangyeob/ACORN_DLKERAS/blob/master/Chapter06/umich_sentiment_lstm.py 를 기반으로 작성되었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/10\n",
      "5668/5668 [==============================] - 20s 4ms/step - loss: 0.2429 - acc: 0.8948 - val_loss: 0.0724 - val_acc: 0.9711\n",
      "Epoch 2/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0466 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0112 - acc: 0.9968 - val_loss: 0.0462 - val_acc: 0.9838\n",
      "Epoch 4/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0036 - acc: 0.9995 - val_loss: 0.0463 - val_acc: 0.9873\n",
      "Epoch 5/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0572 - val_acc: 0.9831\n",
      "Epoch 6/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0494 - val_acc: 0.9866\n",
      "Epoch 7/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0597 - val_acc: 0.9859\n",
      "Epoch 8/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0581 - val_acc: 0.9873\n",
      "Epoch 9/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 0.0010 - acc: 0.9996 - val_loss: 0.0567 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "5668/5668 [==============================] - 19s 3ms/step - loss: 7.0190e-04 - acc: 0.9996 - val_loss: 0.0618 - val_acc: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================] - 0s 210us/step\n",
      "Test score: 0.062, accuracy: 0.989\n",
      "0\t0\teveryone knows brokeback mountain is going to win all because of the stupid gay cowboys .\n",
      "1\t1\tlove luv lubb the da vinci code !\n",
      "1\t1\tlove luv lubb the da vinci code !\n",
      "0\t0\ti hate harry potter .\n",
      "0\t0\tso brokeback mountain was really depressing .\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# 학습 데이터를 읽고 사전을 만든다.\n",
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'r')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()\n",
    "\n",
    "vocab_size = len(word_freqs) + 2\n",
    "word2index = {x[0]: i+2 for i, x in \n",
    "                enumerate(word_freqs.most_common(vocab_size))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "# 문장을 시퀀스로 변환\n",
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'r')\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        #if word2index.has_key(word):\n",
    "        if word in word2index:\n",
    "            seqs.append(word2index[word])\n",
    "        else:\n",
    "            seqs.append(word2index[\"UNK\"])\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "\n",
    "X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, \n",
    "                                                random_state=42)\n",
    "\n",
    "# 모델 작성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(Xtest, ytest))\n",
    "\n",
    "# 손실과 정확도 시각화\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 평가\n",
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,42)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
